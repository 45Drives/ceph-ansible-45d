---
- name: check and deploy filesystem pools
  delegate_to: "{{ groups[mon_group_name][0] }}"
  block:
    - name: check if filesystem pool already exists
      block:
        - name: compile a list of pool names
          set_fact:
            cephfs_pool_names: "{{ cephfs_pools | map(attribute='name') | list }}"

        - name: get and store list of filesystem pools
          command: "{{ container_exec_cmd | default('') }} ceph --cluster {{ cluster }} osd pool ls"
          changed_when: false
          register: osd_pool_ls

        - name: look whether pools to be created are present in the output
          set_fact:
            fs_pools_created: True
          when: osd_pool_ls.stdout_lines | intersect(cephfs_pool_names) | length > 0

    - name: deploy filesystem pools
      when: fs_pools_created is not defined
      block:
        - name: create filesystem pools
          command: >
            {{ hostvars[groups[mon_group_name][0]]['container_exec_cmd'] | default('') }} ceph --cluster {{ cluster }}
            osd pool create {{ item.name }}
            {{ item.pg_num | default(item.pgs) | default(osd_pool_default_pg_num) }}
            {{ item.pgp_num | default(item.pgs) | default(item.pg_num) | default(osd_pool_default_pg_num) }}
            {{ 'replicated_rule' if not item.rule_name | default('replicated_rule') else item.rule_name | default('replicated_rule') }}
            {{ 1 if item.type|default(1) == 'replicated' else 3 if item.type|default(1) == 'erasure' else item.type|default(1) }}
            {%- if (item.type | default("1") == '3' or item.type | default("1") == 'erasure') and item.erasure_profile != '' %}
            {{ item.erasure_profile }}
            {%- endif %}
            {{ item.expected_num_objects | default('') }}
          changed_when: false
          with_items:
            - "{{ cephfs_pools }}"

        - name: customize pool size
          command: "{{ container_exec_cmd | default('') }} ceph --cluster {{ cluster }} osd pool set {{ item.name }} size {{ item.size | default(osd_pool_default_size) }}"
          with_items: "{{ cephfs_pools | unique }}"
          changed_when: false
          when: item.size | default(osd_pool_default_size) != ceph_osd_pool_default_size

        - name: customize pool min_size
          command: "{{ container_exec_cmd | default('') }} ceph --cluster {{ cluster }} osd pool set {{ item.name }} min_size {{ item.min_size | default(osd_pool_default_min_size) }}"
          with_items: "{{ cephfs_pools | unique }}"
          changed_when: false
          when: (item.min_size | default(osd_pool_default_min_size))|int > ceph_osd_pool_default_min_size

        - name: assign application to cephfs pools
          command: "{{ container_exec_cmd | default('') }} ceph --cluster {{ cluster }} osd pool application enable {{ item }} cephfs"
          with_items:
            - "{{ cephfs_data }}"
            - "{{ cephfs_metadata }}"
          changed_when: false

- name: check and create ceph filesystem
  delegate_to: "{{ groups[mon_group_name][0] }}"
  block:
    - name: check if ceph filesystem already exists
      command: "{{ container_exec_cmd | default('') }} ceph --cluster {{ cluster }} fs get {{ cephfs }}"
      register: check_existing_cephfs
      changed_when: false
      failed_when: false

    - name: create ceph filesystem
      command: "{{ container_exec_cmd | default('') }} ceph --cluster {{ cluster }} fs new {{ cephfs }} {{ cephfs_metadata }} {{ cephfs_data }}"
      changed_when: false
      when: check_existing_cephfs.rc != 0

- name: set max_mds
  command: "{{ container_exec_cmd | default('') }} ceph --cluster {{ cluster }} fs set {{ cephfs }} max_mds {{ mds_max_mds }}"
  changed_when: false
  delegate_to: "{{ groups[mon_group_name][0] }}"
  when: mds_max_mds > 1

- name: set max_file_size
  command: "{{ container_exec_cmd | default('') }} ceph --cluster {{ cluster }} fs set {{ cephfs }} max_file_size {{ mds_max_file_size }}"
  changed_when: false
  delegate_to: "{{ groups[mon_group_name][0] }}"
  when: mds_max_file_size is defined

- name: enable standby-replay when more than one mds server
  command: "{{ ner_exec_cmd | default('') }} ceph --cluster {{ cluster }} fs set {{ cephfs }} allow_standby_replay true"
  changed_when: false
  delegate_to: "{{ groups[mon_group_name][0] }}"
  when: groups.get(mds_group_name, []) | length > 1

- name: set minimum standby mds daemons
  command: "{{ ner_exec_cmd | default('') }} ceph --cluster {{ cluster }} fs set {{ cephfs }} standby_count_wanted {{ mds_min_standby_daemons }}"
  changed_when: false
  delegate_to: "{{ groups[mon_group_name][0] }}"
  when: groups.get(mds_group_name, []) | length > 1